{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f83b3f4-16e0-4419-8dd9-4aa8611bcaf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Michelin Restaurant Dataset: Encoding Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8971ed1-3894-4b8d-b8c9-b2b6e8bc8d11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from transformers import AutoTokenizer, OpenAIGPTTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cd14fdd-baeb-4fcf-86f4-a469eb4389ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Catalog, Schema Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0900a1ed-ed9d-4e43-82ca-e5680c982f19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_ = os.getenv('CATALOG_NAME')\n",
    "schema_ = os.getenv('SCHEMA_NAME')\n",
    "spark.sql(\"USE CATALOG \"+catalog_)\n",
    "spark.sql(\"USE SCHEMA \"+schema_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b07fbe8-2f88-4633-8efa-c0c4c41969af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read Silver Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8636e0c-8024-441d-ba15-4598bfad75d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = spark.sql(\"SELECT * FROM silver_data\")\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6aca511-337f-4f2c-ae2e-c19ee7f58425",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Restaurant Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e51a541d-c4aa-4f30-8c6b-c31635216abb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Tokenization\n",
    "Creating a Spark UDF that tokenizes the descriptions, exploding each token to a new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369e35f5-ebb0-4bd2-a6fd-f3e33c96089e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Base function for splitting and tokenizing (splitting not needed here)\n",
    "max_chunk_size = 300\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)\n",
    "\n",
    "def tokz_(text, min_chunk_size = 1, max_chunk_size=max_chunk_size):\n",
    "  if not text:\n",
    "    return []\n",
    "  chunks = text_splitter.split_text(text)\n",
    "  return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6e64f1-ae37-49fe-9f49-bbbf3bca9ec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Encapsulate function into a PandasUDF\n",
    "@pandas_udf(\"array<string>\")\n",
    "def save_tokens(descr: pd.Series) -> pd.Series:\n",
    "  return descr.apply(tokz_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f0a246-6dc7-4ed6-b82e-850c8387464a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create an empty table with tokens\n",
    "DROP TABLE IF EXISTS rest_descr_tokenized;\n",
    "CREATE TABLE IF NOT EXISTS rest_descr_tokenized (\n",
    "  Id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  Res_ID STRING,\n",
    "  Descr_Tokenized STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e920c327-9753-4bf5-932a-e9d921346f6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Generating Tokens for all descriptions\n",
    "(spark.table(\"silver_data\")\n",
    "        .filter('Description is not null')\n",
    "        .select(['Res_ID', 'Description'])\n",
    "        .withColumn('Descr_Tokenized', explode(save_tokens('Description')))\n",
    "        .drop('Description')\n",
    "        .write\n",
    "        .mode('overwrite')\n",
    "        .saveAsTable('rest_descr_tokenized'))\n",
    "\n",
    "display(spark.table('rest_descr_tokenized'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "622b02d4-e814-4246-ac46-24b20c737d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Embedding\n",
    "Starting from the different sentences, embed them using a BGE model.\n",
    "\n",
    "The encode method is explained [here](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd06494-6170-451d-956d-200f228a7602",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Create a base function to embed sentences\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the BAAI/bge-m3 model\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Embed\n",
    "def embed(text_):\n",
    "  return model.encode(text_)\n",
    "\n",
    "# Output example\n",
    "emb = Embed_(\"This is an example\")\n",
    "print(emb)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2f426d-2483-416a-8562-da80243a9b31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Wrap the function into a Spark UDF\n",
    "@pandas_udf(\"array<float>\")\n",
    "def save_embeddings(text_: pd.Series) -> pd.Series:\n",
    "  return text_.apply(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259f691b-5fc3-4b36-bd18-c7b1e93fb113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create an empty table with embeddings\n",
    "DROP TABLE IF EXISTS rest_descr_embedded;\n",
    "CREATE TABLE IF NOT EXISTS rest_descr_embedded (\n",
    "  Id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  Res_ID STRING,\n",
    "  Descr_Tokenized STRING,\n",
    "  Descr_Embedded ARRAY<FLOAT>\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb51b4a8-3103-4556-a1e5-de4100f5908f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Generating Embeddings\n",
    "(spark.table(\"rest_descr_tokenized\")\n",
    "        .select(['Res_ID', 'Descr_Tokenized'])\n",
    "        .withColumn('Descr_Embedded', save_embeddings('Descr_Tokenized'))\n",
    "        .write\n",
    "        .mode('overwrite')\n",
    "        .saveAsTable('rest_descr_embedded'))\n",
    "\n",
    "display(spark.table('rest_descr_embedded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df5fcaac-dfcd-4b80-98a8-3130b06404f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1284968239717256,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Michelin_Encoding",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

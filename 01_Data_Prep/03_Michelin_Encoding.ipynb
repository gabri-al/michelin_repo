{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f83b3f4-16e0-4419-8dd9-4aa8611bcaf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Michelin Restaurant Dataset: Encoding Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8971ed1-3894-4b8d-b8c9-b2b6e8bc8d11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from transformers import AutoTokenizer, OpenAIGPTTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import mlflow\n",
    "import mlflow.sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd14fdd-baeb-4fcf-86f4-a469eb4389ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Catalog, Schema Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0900a1ed-ed9d-4e43-82ca-e5680c982f19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_ = os.getenv('CATALOG_NAME')\n",
    "schema_ = os.getenv('SCHEMA_NAME')\n",
    "spark.sql(\"USE CATALOG \"+catalog_)\n",
    "spark.sql(\"USE SCHEMA \"+schema_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b07fbe8-2f88-4633-8efa-c0c4c41969af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read Silver Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8636e0c-8024-441d-ba15-4598bfad75d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = spark.sql(\"SELECT * FROM silver_data\")\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a57d9a-821e-47d4-a1eb-b059917bbad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*), count(distinct Res_ID)\n",
    "from silver_data\n",
    "where Description is not null;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d06c8ad-0893-4954-9d98-0ed8061d86f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Export Silver Data to Parquet\n",
    "vol_ = \"/Volumes/michelin_ga/restaurants/init/silver_data.parquet\"\n",
    "(silver_df\n",
    "      .write\n",
    "      .format(\"parquet\")\n",
    "      .mode(\"overwrite\")\n",
    "      .save(vol_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6aca511-337f-4f2c-ae2e-c19ee7f58425",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Encode Restaurant Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51a541d-c4aa-4f30-8c6b-c31635216abb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Tokenization\n",
    "Creating a Spark UDF that tokenizes the descriptions, exploding each token to a new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369e35f5-ebb0-4bd2-a6fd-f3e33c96089e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Base function for splitting and tokenizing\n",
    "max_chunk_size = 200\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)\n",
    "\n",
    "def tokz_(text, min_chunk_size = 1, max_chunk_size=max_chunk_size):\n",
    "  if not text:\n",
    "    return []\n",
    "  chunks = text_splitter.split_text(text)\n",
    "  return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6e64f1-ae37-49fe-9f49-bbbf3bca9ec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Encapsulate function into a PandasUDF\n",
    "@pandas_udf(\"array<string>\")\n",
    "def save_tokens(descr: pd.Series) -> pd.Series:\n",
    "  return descr.apply(tokz_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f0a246-6dc7-4ed6-b82e-850c8387464a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create an empty table with tokens\n",
    "DROP TABLE IF EXISTS rest_descr_tokenized;\n",
    "CREATE TABLE IF NOT EXISTS rest_descr_tokenized (\n",
    "  Id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  Res_ID STRING,\n",
    "  Descr_Tokenized STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e920c327-9753-4bf5-932a-e9d921346f6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Generating Tokens for all descriptions\n",
    "(spark.table(\"silver_data\")\n",
    "        .filter('Description is not null') # This removes one record\n",
    "        .select(['Res_ID', 'Description'])\n",
    "        .withColumn('Descr_Tokenized', explode(save_tokens('Description')))\n",
    "        .drop('Description')\n",
    "        .write\n",
    "        .mode('overwrite')\n",
    "        .saveAsTable('rest_descr_tokenized'))\n",
    "\n",
    "display(spark.table('rest_descr_tokenized'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0570d122-d206-496c-8349-2aa81d2c19cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) tot_recs, count(distinct Res_ID) unique_res_Id, min(Res_ID) Res_ID_min, max(Res_ID) Res_ID_max\n",
    "from rest_descr_tokenized;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622b02d4-e814-4246-ac46-24b20c737d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Embedding\n",
    "Starting from the different sentences, embed them using a BGE model.\n",
    "- The BAAI/bge-m3 model was tested but it is designed to return similarity scores, not ideal for pure embedding\n",
    "- The \"bge-base-en-v1.5\" embedding model is more indicated and available through Huggingface API [here](https://huggingface.co/BAAI/bge-base-en-v1.5)\n",
    "\n",
    "For Huggingface models:\n",
    "- `encode` method is explained [here](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c94cb3-27a2-492c-96c9-14b24f5320d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create and register model\n",
    "\n",
    "### Import the model\n",
    "# model = SentenceTransformer(\"BAAI/bge-m3\") # This model is designed to return a set of similarity scores, not doing pure embedding\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "### Generate signature\n",
    "data_ex = spark.sql(\"Select Descr_Tokenized from rest_descr_tokenized limit 3\").collect()\n",
    "input_ex = [row['Descr_Tokenized'] for row in data_ex]\n",
    "output_ex = model.encode(input_ex)\n",
    "signature = mlflow.models.infer_signature(\n",
    "    model_input=input_ex,\n",
    "    model_output=output_ex,\n",
    ")\n",
    "\n",
    "# Log & Register the model\n",
    "model_name_ = f\"{catalog_}.{schema_}.ga_bge_base_en\"\n",
    "with mlflow.start_run(run_name=\"Embedding_Model\"):\n",
    "    logged_model = mlflow.sentence_transformers.log_model(\n",
    "        model = model,\n",
    "        registered_model_name = model_name_,\n",
    "        artifact_path = \"ga_bge_base_en\",\n",
    "        signature = signature,\n",
    "        input_example = input_ex,\n",
    "    )\n",
    "\n",
    "loaded_model = mlflow.sentence_transformers.load_model(logged_model.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd06494-6170-451d-956d-200f228a7602",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Local Model\n",
    "# Load model\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name__ = \"models:/ga_bge_base_en/2\"\n",
    "loaded_model = mlflow.sentence_transformers.load_model(model_name__)\n",
    "\n",
    "# Define function\n",
    "def embed_locally(text_):\n",
    "  return loaded_model.encode(text_)\n",
    "\n",
    "# Testing\n",
    "emb = embed_locally(\"This is an example\")\n",
    "print(emb)\n",
    "print(emb.shape) # Array of 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e086c8d-41dc-450d-a91b-d245b3a22250",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Embed from Endpoint [This is configured with the BAAI/bge-m3 model]\n",
    "endpoint_url = 'https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/ga_embedding/invocations'\n",
    "scope_name_ = 'michelin_scope'\n",
    "secret_name_ = 'pat_ga'\n",
    "PAT_ = dbutils.secrets.get(scope=scope_name_, key=secret_name_)\n",
    "\n",
    "def embed(text_):\n",
    "    headers = {'Authorization': f'Bearer {PAT_}'}\n",
    "    data_dict = {\"inputs\": [text_]}\n",
    "    response = requests.post(url=endpoint_url, headers=headers, json=data_dict)\n",
    "    if response.status_code == 200:\n",
    "      embedded_data = response.json()['predictions'][0]\n",
    "      #print(\":: Successful!\")\n",
    "      #print(embedded_data)\n",
    "    else:\n",
    "      embedded_data = None\n",
    "      print(\"Failed! Status code: %s\" % (response.status_code))\n",
    "    return embedded_data\n",
    "\n",
    "# Testing\n",
    "emb = embed(\"This is an example\")\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2f426d-2483-416a-8562-da80243a9b31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Wrap the function into a Spark UDF\n",
    "@pandas_udf(\"array<float>\")\n",
    "def save_embeddings(text_: pd.Series) -> pd.Series:\n",
    "  #return text_.apply(embed)\n",
    "  return text_.apply(embed_locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259f691b-5fc3-4b36-bd18-c7b1e93fb113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create an empty table with embeddings [This table contains embeddings based on the BAAI/bge-m3 model]\n",
    "-- DROP TABLE IF EXISTS rest_descr_embedded;\n",
    "/*\n",
    "CREATE TABLE IF NOT EXISTS rest_descr_embedded (\n",
    "  Id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  Res_ID STRING,\n",
    "  Descr_Tokenized STRING,\n",
    "  Descr_Embedded ARRAY<FLOAT>\n",
    ");*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f804d9-62b9-44ff-a9d0-5a6a3058e010",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create an empty table with embeddings [This table contains embeddings based on the BAAI/bge-base-en-v1.5 model]\n",
    "DROP TABLE IF EXISTS gold_rest_descr_embedded;\n",
    "CREATE TABLE IF NOT EXISTS gold_rest_descr_embedded (\n",
    "  Id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  Res_ID STRING,\n",
    "  Descr_Tokenized STRING,\n",
    "  Descr_Embedded ARRAY<FLOAT>\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb51b4a8-3103-4556-a1e5-de4100f5908f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Generating Embeddings -- Appending new data proceeding in bulks\n",
    "\n",
    "# Creating a bulk (2k records each)\n",
    "spark.sql(\"\"\"\n",
    "          SELECT * from rest_descr_tokenized where \n",
    "          int(right(Res_ID, 5)) >= 12001 and int(right(Res_ID, 5)) <= 16000\n",
    "          \"\"\").createOrReplaceTempView(\"bulk_v\") # 11 mins for 2k records\n",
    "\n",
    "# Applying the function\n",
    "(spark.table(\"bulk_v\")\n",
    "        .select(['Res_ID', 'Descr_Tokenized'])\n",
    "        .withColumn('Descr_Embedded', save_embeddings('Descr_Tokenized'))\n",
    "        .write\n",
    "        .mode('append')\n",
    "        #.saveAsTable('rest_descr_embedded')\n",
    "        .saveAsTable('gold_rest_descr_embedded'))\n",
    "\n",
    "#display(spark.table('rest_descr_embedded'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae322e50-96ea-4d77-9efe-e34b61b71ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f712ce-e5dd-4dcf-aec2-3cd860a8a252",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify Tot Counts\n",
    "select count(*) tot_recs, count(distinct Res_ID) Res_ID_uniques, min(Res_ID), max(Res_ID)\n",
    "from gold_rest_descr_embedded;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40481f8a-239e-47d5-b1d8-049dbd8ba19b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) tot_recs, count(distinct Res_ID) Res_ID_uniques, min(Res_ID), max(Res_ID)\n",
    "from rest_descr_tokenized;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a593e1e5-e304-4d99-95e3-9acac8945dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify double counts\n",
    "with d0 as (select Res_Id, count(*) from gold_rest_descr_embedded group by 1 having count(*) > 1)\n",
    "select Res_Id, Descr_Tokenized\n",
    "from gold_rest_descr_embedded where Res_Id in (select distinct Res_Id from d0)\n",
    "order by 1,2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183e0dcd-3958-4bb4-858c-75651db07f24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify double counts\n",
    "with d0 as (select Res_Id, count(*) from rest_descr_tokenized group by 1 having count(*) > 1)\n",
    "select Res_Id, Descr_Tokenized\n",
    "from rest_descr_tokenized where Res_Id in (select distinct Res_Id from d0)\n",
    "order by 1,2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40454149-cd02-42b2-94ee-2385bf31e619",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2794f14a-2598-4f63-b279-650436195d60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to parquet to vol (csv doesn't support arrays)\n",
    "vol_ = \"/Volumes/michelin_ga/restaurants/init/gold_embedded_data.parquet\"\n",
    "\n",
    "emd_data = spark.table(\"gold_rest_descr_embedded\")\n",
    "\n",
    "(emd_data\n",
    "      .write\n",
    "      .format(\"parquet\")\n",
    "      .mode(\"overwrite\")\n",
    "      .save(vol_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb26563-b8cd-4d8b-b665-2edd089ddd3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Testing Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032f54eb-4112-49fa-a2f2-081933aba4e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Upload data in memory using pandas (single node infra, like in a webapp)\n",
    "path_ = \"/Volumes/michelin_ga/restaurants/init/gold_embedded_data.parquet\"\n",
    "\n",
    "rest_descr_df = pd.read_parquet(path_)\n",
    "display(rest_descr_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41e84a2-bd0b-4c9a-b2d6-00adbe97f3ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Prepare a numpy matrix of all restaurants embeddings\n",
    "emb_len = 768 #1024\n",
    "res_ids = []\n",
    "res_emb = np.zeros((len(rest_descr_df), emb_len), dtype=float)\n",
    "i = 0\n",
    "for index, row in rest_descr_df.iterrows():\n",
    "  res_ids.append(row['Res_ID'])\n",
    "  res_emb[i] = row['Descr_Embedded']\n",
    "  i += 1\n",
    "\n",
    "print(len(res_ids))\n",
    "print(res_emb.shape)\n",
    "print(res_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7d8729-fecc-45ac-92fc-5c1ba3d063e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Test similarity retrieval with a sample sentence\n",
    "N = 10 # Return N top restaurants\n",
    "#sample_ = \"I am looking for a modern vegan restaurant with a futuristic atmosphere.\"\n",
    "#sample_ = \"I'd like to eat at any former culinary TV show contestant's restaurant.\"\n",
    "sample_ = \"I'd like to find a restaurant from any MasterChef winner.\"\n",
    "\n",
    "## Encode sample sentence\n",
    "sample_enb = np.array(embed_locally(sample_))\n",
    "#print(sample_enb.shape)\n",
    "\n",
    "## Calculate cos similarity\n",
    "Cosine_sim = np.dot(res_emb,sample_enb)/(norm(res_emb, axis=1)*norm(sample_enb))\n",
    "Cosine_sim_df = pd.DataFrame({'Res_IDs': res_ids, 'Cosine_Similarity': Cosine_sim})\n",
    "Cosine_sim_df = Cosine_sim_df.groupby('Res_IDs').agg(Cosine_Similarity_Max = ('Cosine_Similarity', 'max')).reset_index()\n",
    "\n",
    "## Join scores with original dataframe\n",
    "silver_pandas_df = silver_df.toPandas()\n",
    "Results_df = pd.merge(left = silver_pandas_df, right = Cosine_sim_df, left_on = 'Res_ID', right_on = 'Res_IDs', how = 'inner')\n",
    "Results_df.sort_values(by = 'Cosine_Similarity_Max', ascending = False, inplace = True)\n",
    "display(Results_df.head(N))\n",
    "\n",
    "#display(len(silver_pandas_df))\n",
    "#display(len(Cosine_sim_df))\n",
    "#display(len(Results_df))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1986678755415356,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Michelin_Encoding",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
